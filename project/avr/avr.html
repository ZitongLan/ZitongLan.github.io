<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="AVR: Acosutic Volume Rendering for Neural Impulse Response Field">
  <meta property="og:title" content="AVR">
  <meta property="og:description" content="AVR: Acosutic Volume Rendering for Neural Impulse Response Field">
  <meta property="twitter:title" content="AVR">
  <meta property="twitter:description" content="AVR: Acosutic Volume Rendering for Neural Impulse Response Field">
  <meta property="og:type" content="website">
  <meta name="keywords" content="Acoustic volume Rendering, Spatial Audio">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>AVR</title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-D65ZW4CJYF"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-D65ZW4CJYF');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script> -->
  <script src="./static/js/jquery-3.6.4.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/lazy.js"></script>
  <script src="./static/js/faster.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">Acoustic Volume Rendering for Neural Impulse Response Field
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://zitonglan.github.io/">Zitong Lan</a><sup>1</sup>, </span>
              <span class="author-block">
                <a href="https://hellomuffin.github.io/">Chenhao Zheng</a><sup>2</sup>, </span>
              <span class="author-block">
                <a href="">Zhiwei Zheng</a><sup>1</sup>, </span>
              <span class="author-block">
                <a href="https://www.cis.upenn.edu/~mingminz/">Mingmin Zhao</a><sup>1</sup>, </span>
              <span class="author-block">
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>University of Pennsylvania &nbsp; </span>
              <span class="author-block"><sup>2</sup>University of Washington &nbsp; </span>
            </div>

            <h1 style="font-size:24px;">
              <span style="color:black;">NeurIPS'24</span>
              <span style="color:red;">Spotlight</span>
            </h1>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <style>
    .video-grid {
      display: grid;
      grid-template-columns: repeat(1, 1fr);
      /* Three columns */
      grid-template-rows: repeat(1, 1fr);
      /* Two rows */
      gap: 0px 4px;
      /* Gap between videos */
      width: 65%;
      /* Set the container width to 80% */
      margin: 0 auto;
      /* Center the container horizontally */
    }

    .video-grid video {
      width: 100%;
      /* Videos fill the container width */
      height: auto;
    }
  </style>

  <div class="container" style="max-width: 40%;">
    <div class="columns is-centered">
      <div style="background-color: #f9f9f9; border-left: 2px solid #3273dc; padding: 10px; margin-bottom: 10px;">
        <p style="font-size:20px;"><strong>Tl;DR:</strong> We propose <b>acoustic volume rendering</b> for
          impulse response rendering. Wave propagation physics is incorporated into the acoustic volume rendering
          to ensure multi-pose consistency for acoustic signal. Our method provide a better method to model the acoustic
          propagation.
      </div>
    </div>
  </div>

  <br><br>

  <!-- Example YouTube video embed -->
  <div class="columns is-centered has-text-centered">
    <h2 class="title is-3"> Presentation Video</h2>
  </div>
  <center><iframe width="720" height="400" src="https://www.youtube.com/embed/NAnhT2frQLo" frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
    </iframe>
  </center>


  <section class="section">
    <div class="container" style="max-width: 50%;">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <!-- <div class="columns is-centered">
            <div style="background-color: #f9f9f9; border-left: 2px solid #3273dc; padding: 10px; margin-bottom: 10px;">
              <p style="font-size:20px;"><strong>Tl;DR:</strong> We propose <b>acoustic volume rendering</b> for
                impulse response rendering. Wave propagation physics is incorporated into the acoustic volume rendering
                to ensure multi-pose consistency for acoustic signal
            </div>
          </div> -->

          <div class="content has-text-justified">
            <p style="font-size:18px;">
              As technologies like Virtual Reality continue to advance, realistic synthesis of spatial sound has become
              crucial for creating immersive experiences.
              Synthesizing the sound received at any position relies on the estimation of impulse response (IR), which
              describes how sound propagates in one scene.
              While various learning-based approaches have been proposed, their generalization to unseen poses remains
              unsatisfactory.
              In this paper, we present <b>A</b>coustic <b>V</b>olume <b>R</b>endering (AVR) for impulse
              response rendering. AVR enables the construction of an impulse response field that follows wave
              propagation principles and achieves state-of-the-art performance in synthesizing impulse responses for
              unseen poses. We introduce frequency-domain signal rendering and spherical integration specifically for
              acoustic volume rendering.
              Experiments show that AVR surpasses the current leading methods by a substantial margin.
              Additionally, we developed an acoustic simulation platform, AcoustiX, which simulates more realistic
              impulse responses than existing simulators.
            </p>
          </div>
        </div>
      </div>
  </section>


  <section>
    <div class="columns is-centered has-text-centered">
      <h2 class="title is-3">Examples of Rendered Audio</h2>
    </div>
    <br>
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="content has-text-justified">
          <p style="font-size:22px;">
            Rendered impulse response from our method on the Real Acoustic Field dataset. <b>Headphones are strongly
              recommended.</b>
          </p>
        </div>
      </div>
    </div>
    </div>
    <br>


    <center> <video width="640" height="360" controls>
        <source src="static/videos/raf_empty_demo.mp4" type="video/mp4">
      </video>
      <video width="640" height="360" controls>
        <source src="static/videos/raf_furnished_demo.mp4" type="video/mp4">
      </video></center>


    <br><br>
    <br>


    <section>
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3"> Zero-Shot Spatial Binaural Audio Rendering</h2>
      </div>
      <br>
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="content has-text-justified">
            <p style="font-size:22px;">Our model can render binaural audio. We can render binaural audio by simply
              synthesizing the sound heard at the left and right
              ears separately. </p>
          </div>
        </div>
      </div>
      </div>

      <br>
      <center> <video width="640" height="360" controls>
          <source src="static/videos/demo_spatial_seeuagain.mp4" type="video/mp4">
        </video></center>
    </section>
    <br><br>

    <div class="video-grid">
      <div class="has-text-centered">
        <h2 class="title is-3 has-text-centered">Overview</h2>

        <div>
          <img src="static/images/teaser_figure.png" class="interpolation-image"
            alt="Interpolate start reference image." />
        </div>
        <div class="content has-text-justified" style="width: 60%; margin: 0 auto;">
          <p style="font-size:22px;"><b>Left: Task illustration.</b> From observations of the sound emitted by a
            speaker, our model constructs an impulse response field that can synthesize observations at any listener
            position.
            <br>
            <b>Right: Waveform visualization</b> We transfer the signal into frequency domain, and we visualize phase
            and amplitude distributions at a specific wavelength. Our method predict correct signal spatial
            distributions.
          </p>
          </p>
        </div>
      </div>
    </div>

    <br>
    <br>
    <div class="video-grid">
      <div class="has-text-centered">
        <h2 class="title is-3 has-text-centered">Method</h2>

        <div>
          <img src="static/images/method.png" class="interpolation-image" alt="Interpolate start reference image." />
        </div>
        <br>
        <div class="content has-text-justified" style="width: 60%; margin: 0 auto;">
          <p style="font-size:22px;">
            <b>Rendering pipeline.</b> We sample points along the ray that is shot from the microphone, and query the
            network to obtain signals and density. Time delay is applied to account for the wave propagation. After
            that, we combine signals and densities to perform acoustic volume rendering for each ray to get the
            directional signal. We integrate along the sphere to combine signals from all possible directions with gain
            pattern to obtain the final rendered impulse response.
        </div>
      </div>
    </div>

    <br> <br>

    <style>
      .video-grid-two-cols {
        display: grid;
        grid-template-columns: repeat(2, 1fr);
        /* Two columns */
        gap: 10px;
        /* Gap between videos */
        width: 60%;
        /* Set the container width to 60% */
        margin: 0 auto;
        /* Center the container horizontally */
      }

      .video-grid-two-cols video {
        width: 100%;
        /* Videos fill the container width */
        height: auto;
      }

      .interpolation-image {
        width: 80%;
        /* Videos fill the container width */
        height: auto;
      }
    </style>

    <br>
    <section>
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">More Qualitative Result</h2>
        <a id="interactive_demo"></a>
      </div>
      <div class="video-grid">
        <div>
          <center> <img src="static/images/channel.png" class="interpolation-image"
              alt="Interpolate start reference image." /> </center>
          <center>
            <p style="font-size: 22px;">Visualization of spatial signal distributions </p>
          </center>
        </div>
        <br><br>

      </div>
      <div class="video-grid">
        <div>
          <center><img src="static/images/wave.png" class="interpolation-image"
              alt="Interpolate start reference image." /></center>
        </div>
        <center>
          <p style="font-size: 22px;">Synthesized impulse response across different methods </p>
        </center>
      </div>
    </section>

    <br>
    <br>
    <br>


    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title is-3">BibTeX</h2>
        <pre><code>

</code></pre>
      </div>
    </section>



    <footer class="footer">
      <div align="center" class="container">
        <div class="columns is-centered">
          <div class="content">
            This website is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
          </div>
        </div>
      </div>
    </footer>


</body>

</html>